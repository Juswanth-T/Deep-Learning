{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnBsQGER6vvQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import Image\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.vgg16 import  preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import ParameterGrid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "def create_model(num_feature_maps_cl2):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional Layer 1 (CL1)\n",
        "    model.add(layers.Conv2D(4, (3, 3), strides=(1, 1), activation='relu', input_shape=(224, 224, 3)))\n",
        "\n",
        "    # Pooling Layer 1 (PL1) - Mean Pooling\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Convolutional Layer 2 (CL2)\n",
        "    model.add(layers.Conv2D(num_feature_maps_cl2, (3, 3), strides=(1, 1), activation='relu'))\n",
        "\n",
        "    # Pooling Layer 2 (PL2) - Mean Pooling\n",
        "    model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    # Output Layer\n",
        "    model.add(layers.Dense(5, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "d338KaCO7U1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")  # VGG16 preprocessin"
      ],
      "metadata": {
        "id": "OmuMAaDJ7YTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir = Path(\"drive/MyDrive/task_1\")\n",
        "train_data_dir = download_dir / \"train\"\n",
        "valid_data_dir = download_dir / \"valid\"\n",
        "test_data_dir = download_dir / \"test\"\n",
        "\n",
        "testgen = test_generator.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    class_mode=None,\n",
        "    shuffle=False,\n",
        "    seed=42,\n",
        "    batch_size = 1,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYZT8v187qh6",
        "outputId": "fe04ff56-62d4-4e62-b397-bce4aad8d824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_acc(model,testgen,batch):\n",
        "# Evaluate the model\n",
        "  test_loss, test_acc = model.evaluate(testgen, steps=(testgen.samples + batch - 1)//batch)\n",
        "  #print(f'Test accuracy: {test_acc}')\n",
        "  return test_acc"
      ],
      "metadata": {
        "id": "OYDS5kVT9J0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure TensorFlow is using the GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "    print(\"TensorFlow is using the GPU\")\n",
        "else:\n",
        "    print(\"TensorFlow is not using the GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM8o2U6iYX8q",
        "outputId": "f5f217a1-921f-4e90-e3be-7c3612f35b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is using the GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming create_model is defined elsewhere\n",
        "def create_model2(num_feature_maps_cl2, lr):\n",
        "    model = create_model(num_feature_maps_cl2)\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "9W36QSiNWR2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'batch_size': [64, 128],\n",
        "    'epochs': [10, 20],\n",
        "    'num_feature_maps_cl2': [10, 20],\n",
        "    'lr': [1e-3, 1e-4]\n",
        "}"
      ],
      "metadata": {
        "id": "C2oyWZjkXKf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to perform the grid search\n",
        "def perform_grid_search(param_grid):\n",
        "    best_score = np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        batch_size = params['batch_size']\n",
        "        epochs = params['epochs']\n",
        "        num_feature_maps_cl2 = params['num_feature_maps_cl2']\n",
        "        lr = params['lr']\n",
        "        train_generator = ImageDataGenerator(\n",
        "            validation_split=0,\n",
        "            preprocessing_function=preprocess_input,\n",
        "        )  # VGG16 preprocessing\n",
        "\n",
        "        traingen = train_generator.flow_from_directory(\n",
        "          train_data_dir,\n",
        "          target_size=(224, 224),\n",
        "          class_mode=\"categorical\",\n",
        "          subset=\"training\",\n",
        "          shuffle=True,\n",
        "          seed=42,\n",
        "          batch_size = batch_size ,\n",
        "        )\n",
        "\n",
        "        train_generator = ImageDataGenerator(\n",
        "            validation_split=0.999,\n",
        "            preprocessing_function=preprocess_input,\n",
        "        )  # VGG16 preprocessing\n",
        "\n",
        "\n",
        "        validgen = train_generator.flow_from_directory(\n",
        "            valid_data_dir,\n",
        "            target_size=(224, 224),\n",
        "            class_mode=\"categorical\",\n",
        "            subset=\"validation\",\n",
        "            shuffle=True,\n",
        "            seed=42,\n",
        "            batch_size = batch_size,\n",
        "\n",
        "        )\n",
        "        print(f\"Training with params: {params}\")\n",
        "\n",
        "        model = create_model2(num_feature_maps_cl2, lr)\n",
        "\n",
        "        # Define the ModelCheckpoint callback\n",
        "        checkpoint_filepath = 'drive/MyDrive/task_1/best_model_task_2.keras'\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            filepath=checkpoint_filepath,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='min',\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(\n",
        "            traingen,\n",
        "            epochs=epochs,\n",
        "            validation_data=validgen,\n",
        "            steps_per_epoch=len(traingen),\n",
        "            validation_steps=len(validgen),\n",
        "            callbacks=[checkpoint_callback],\n",
        "            verbose=1,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Get the best validation loss for this model\n",
        "        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "        if val_loss < best_score:\n",
        "            best_score = val_loss\n",
        "            best_params = params\n",
        "            best_model = model  # You might want to load the best saved model instead\n",
        "\n",
        "    print(f\"Best score: {best_score} with params: {best_params}\")\n",
        "    return best_model, best_params\n"
      ],
      "metadata": {
        "id": "jqR26T0BXSrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, best_params = perform_grid_search(param_grid)"
      ],
      "metadata": {
        "id": "ylMJLGmYBbri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4998918b-05f8-4f3b-9843-1f6d84e46583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 10, 'lr': 0.001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 1212s 39s/step - loss: 1.6590 - accuracy: 0.2899 - val_loss: 1.9528 - val_accuracy: 0.2707\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 16s 503ms/step - loss: 1.3544 - accuracy: 0.4388 - val_loss: 1.4748 - val_accuracy: 0.3980\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 13s 415ms/step - loss: 1.0962 - accuracy: 0.5797 - val_loss: 1.5308 - val_accuracy: 0.3212\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 13s 413ms/step - loss: 0.6653 - accuracy: 0.7796 - val_loss: 1.4658 - val_accuracy: 0.4182\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 13s 416ms/step - loss: 0.2934 - accuracy: 0.9305 - val_loss: 1.4566 - val_accuracy: 0.3657\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 13s 412ms/step - loss: 0.1142 - accuracy: 0.9880 - val_loss: 1.4589 - val_accuracy: 0.3879\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 14s 424ms/step - loss: 0.0431 - accuracy: 0.9975 - val_loss: 1.4801 - val_accuracy: 0.3596\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 0.0269 - accuracy: 0.9990 - val_loss: 1.4440 - val_accuracy: 0.4182\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 13s 401ms/step - loss: 0.0155 - accuracy: 0.9995 - val_loss: 1.4555 - val_accuracy: 0.4222\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.0126 - accuracy: 0.9995 - val_loss: 1.4386 - val_accuracy: 0.4323\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 10, 'lr': 0.001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 17s 431ms/step - loss: 1.5853 - accuracy: 0.3568 - val_loss: 1.8208 - val_accuracy: 0.3051\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 14s 430ms/step - loss: 1.2531 - accuracy: 0.4948 - val_loss: 1.7574 - val_accuracy: 0.2121\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 0.9839 - accuracy: 0.6307 - val_loss: 1.4923 - val_accuracy: 0.3253\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.5536 - accuracy: 0.8441 - val_loss: 1.5725 - val_accuracy: 0.3434\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 13s 412ms/step - loss: 0.2088 - accuracy: 0.9585 - val_loss: 1.6761 - val_accuracy: 0.3333\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 13s 412ms/step - loss: 0.0673 - accuracy: 0.9950 - val_loss: 1.5504 - val_accuracy: 0.4323\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 13s 412ms/step - loss: 0.0247 - accuracy: 0.9995 - val_loss: 1.5221 - val_accuracy: 0.4242\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 17s 523ms/step - loss: 0.0207 - accuracy: 0.9985 - val_loss: 1.3419 - val_accuracy: 0.4626\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 0.0124 - accuracy: 0.9995 - val_loss: 1.3771 - val_accuracy: 0.4848\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 1.4255 - val_accuracy: 0.5091\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 10, 'lr': 0.0001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 18s 496ms/step - loss: 1.6822 - accuracy: 0.3518 - val_loss: 1.9780 - val_accuracy: 0.2424\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 1.3123 - accuracy: 0.4663 - val_loss: 1.5362 - val_accuracy: 0.3232\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 13s 422ms/step - loss: 1.1242 - accuracy: 0.5612 - val_loss: 1.4776 - val_accuracy: 0.3838\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.9629 - accuracy: 0.6487 - val_loss: 1.4459 - val_accuracy: 0.4162\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 14s 428ms/step - loss: 0.8096 - accuracy: 0.7396 - val_loss: 1.4104 - val_accuracy: 0.4485\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 13s 413ms/step - loss: 0.6522 - accuracy: 0.8111 - val_loss: 1.4292 - val_accuracy: 0.4242\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 14s 431ms/step - loss: 0.5314 - accuracy: 0.8631 - val_loss: 1.4265 - val_accuracy: 0.4424\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 13s 419ms/step - loss: 0.4356 - accuracy: 0.9110 - val_loss: 1.4122 - val_accuracy: 0.4323\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.3680 - accuracy: 0.9315 - val_loss: 1.4237 - val_accuracy: 0.4566\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.2634 - accuracy: 0.9660 - val_loss: 1.5312 - val_accuracy: 0.4182\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 10, 'lr': 0.0001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 19s 515ms/step - loss: 1.6968 - accuracy: 0.3068 - val_loss: 2.0047 - val_accuracy: 0.2424\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 1.2922 - accuracy: 0.4708 - val_loss: 1.6373 - val_accuracy: 0.3636\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 13s 419ms/step - loss: 1.0887 - accuracy: 0.5947 - val_loss: 1.5332 - val_accuracy: 0.3879\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 13s 413ms/step - loss: 0.8675 - accuracy: 0.7071 - val_loss: 1.4098 - val_accuracy: 0.4323\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 14s 432ms/step - loss: 0.6694 - accuracy: 0.7971 - val_loss: 1.3646 - val_accuracy: 0.4929\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 13s 415ms/step - loss: 0.4896 - accuracy: 0.8711 - val_loss: 1.4598 - val_accuracy: 0.4061\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 14s 427ms/step - loss: 0.3415 - accuracy: 0.9400 - val_loss: 1.4570 - val_accuracy: 0.3919\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.2274 - accuracy: 0.9740 - val_loss: 1.3688 - val_accuracy: 0.4444\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 13s 405ms/step - loss: 0.1445 - accuracy: 0.9880 - val_loss: 1.4237 - val_accuracy: 0.4222\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 16s 509ms/step - loss: 0.1131 - accuracy: 0.9950 - val_loss: 1.3623 - val_accuracy: 0.4545\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 20, 'lr': 0.001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 24s 662ms/step - loss: 1.5424 - accuracy: 0.3483 - val_loss: 1.8257 - val_accuracy: 0.3030\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 14s 428ms/step - loss: 1.1943 - accuracy: 0.5252 - val_loss: 1.4469 - val_accuracy: 0.3778\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 14s 437ms/step - loss: 0.9423 - accuracy: 0.6527 - val_loss: 1.4031 - val_accuracy: 0.4525\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 14s 431ms/step - loss: 0.6135 - accuracy: 0.8111 - val_loss: 1.4515 - val_accuracy: 0.4323\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 13s 423ms/step - loss: 0.2872 - accuracy: 0.9300 - val_loss: 1.5315 - val_accuracy: 0.3677\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 0.1118 - accuracy: 0.9895 - val_loss: 1.4825 - val_accuracy: 0.3960\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 14s 424ms/step - loss: 0.0564 - accuracy: 0.9950 - val_loss: 1.5230 - val_accuracy: 0.3960\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 13s 423ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 1.7816 - val_accuracy: 0.4000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 14s 428ms/step - loss: 0.0297 - accuracy: 0.9980 - val_loss: 1.5606 - val_accuracy: 0.3980\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 14s 438ms/step - loss: 0.0209 - accuracy: 0.9980 - val_loss: 1.6037 - val_accuracy: 0.4303\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 14s 436ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 1.6245 - val_accuracy: 0.4707\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 1.6814 - val_accuracy: 0.4626\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 13s 422ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.8949 - val_accuracy: 0.4424\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.0069 - accuracy: 0.9995 - val_loss: 1.8282 - val_accuracy: 0.5051\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 1.9340 - val_accuracy: 0.4869\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.0054 - accuracy: 0.9995 - val_loss: 2.1562 - val_accuracy: 0.4364\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 13s 415ms/step - loss: 0.0092 - accuracy: 0.9995 - val_loss: 2.6148 - val_accuracy: 0.4081\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 13s 419ms/step - loss: 0.0057 - accuracy: 0.9995 - val_loss: 2.1168 - val_accuracy: 0.4646\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.2031 - val_accuracy: 0.4646\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 2.2241 - val_accuracy: 0.4626\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 20, 'lr': 0.001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 22s 603ms/step - loss: 1.6594 - accuracy: 0.3138 - val_loss: 1.6020 - val_accuracy: 0.2889\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 19s 597ms/step - loss: 1.3132 - accuracy: 0.4568 - val_loss: 1.5371 - val_accuracy: 0.2889\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 1.0736 - accuracy: 0.5802 - val_loss: 1.4903 - val_accuracy: 0.3354\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 0.7810 - accuracy: 0.7126 - val_loss: 1.8567 - val_accuracy: 0.2646\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 13s 413ms/step - loss: 0.4270 - accuracy: 0.8736 - val_loss: 1.6493 - val_accuracy: 0.2566\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.1850 - accuracy: 0.9605 - val_loss: 1.5909 - val_accuracy: 0.2747\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 21s 660ms/step - loss: 0.0798 - accuracy: 0.9870 - val_loss: 1.4543 - val_accuracy: 0.3657\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.0477 - accuracy: 0.9945 - val_loss: 1.6579 - val_accuracy: 0.3495\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.0215 - accuracy: 0.9980 - val_loss: 1.5510 - val_accuracy: 0.3980\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.0168 - accuracy: 0.9990 - val_loss: 1.5461 - val_accuracy: 0.4263\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 0.0096 - accuracy: 0.9995 - val_loss: 1.4635 - val_accuracy: 0.4606\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 14s 426ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.5646 - val_accuracy: 0.4545\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 13s 410ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 1.5912 - val_accuracy: 0.4606\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.5690 - val_accuracy: 0.4626\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.7897 - val_accuracy: 0.4606\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 13s 422ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 1.7102 - val_accuracy: 0.4626\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 13s 400ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.7620 - val_accuracy: 0.4747\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 13s 415ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.7777 - val_accuracy: 0.4687\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.1687 - val_accuracy: 0.4303\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 14s 440ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 2.2728 - val_accuracy: 0.4505\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 20, 'lr': 0.0001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 23s 641ms/step - loss: 1.7673 - accuracy: 0.3083 - val_loss: 3.0080 - val_accuracy: 0.2566\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 14s 435ms/step - loss: 1.4062 - accuracy: 0.4308 - val_loss: 1.9765 - val_accuracy: 0.2869\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 1.2367 - accuracy: 0.5082 - val_loss: 1.6040 - val_accuracy: 0.3556\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 14s 429ms/step - loss: 1.0856 - accuracy: 0.5872 - val_loss: 1.4740 - val_accuracy: 0.3798\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.9489 - accuracy: 0.6707 - val_loss: 1.4459 - val_accuracy: 0.3818\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.8261 - accuracy: 0.7191 - val_loss: 1.4270 - val_accuracy: 0.3838\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 13s 417ms/step - loss: 0.6663 - accuracy: 0.8146 - val_loss: 1.5397 - val_accuracy: 0.3737\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 14s 438ms/step - loss: 0.6011 - accuracy: 0.8346 - val_loss: 1.4816 - val_accuracy: 0.3879\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 13s 405ms/step - loss: 0.4664 - accuracy: 0.8891 - val_loss: 1.4335 - val_accuracy: 0.4081\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.3888 - accuracy: 0.9340 - val_loss: 1.6262 - val_accuracy: 0.3758\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 13s 416ms/step - loss: 0.3076 - accuracy: 0.9515 - val_loss: 1.5072 - val_accuracy: 0.4061\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 13s 408ms/step - loss: 0.2624 - accuracy: 0.9625 - val_loss: 1.4535 - val_accuracy: 0.4162\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 13s 398ms/step - loss: 0.1948 - accuracy: 0.9835 - val_loss: 1.5382 - val_accuracy: 0.3980\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 14s 424ms/step - loss: 0.1628 - accuracy: 0.9890 - val_loss: 1.6832 - val_accuracy: 0.4343\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 13s 424ms/step - loss: 0.1413 - accuracy: 0.9890 - val_loss: 1.8160 - val_accuracy: 0.3455\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.1182 - accuracy: 0.9920 - val_loss: 1.6993 - val_accuracy: 0.3697\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 13s 399ms/step - loss: 0.0901 - accuracy: 0.9975 - val_loss: 1.6772 - val_accuracy: 0.3899\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 13s 409ms/step - loss: 0.0727 - accuracy: 0.9995 - val_loss: 1.7592 - val_accuracy: 0.3859\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 13s 411ms/step - loss: 0.0644 - accuracy: 0.9995 - val_loss: 1.7576 - val_accuracy: 0.4242\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 14s 423ms/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 1.7148 - val_accuracy: 0.4263\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 64, 'epochs': 20, 'lr': 0.0001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 21s 579ms/step - loss: 1.6088 - accuracy: 0.3708 - val_loss: 3.6912 - val_accuracy: 0.2020\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 1.2193 - accuracy: 0.5192 - val_loss: 1.9128 - val_accuracy: 0.2606\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 14s 427ms/step - loss: 1.0089 - accuracy: 0.6292 - val_loss: 1.4784 - val_accuracy: 0.4323\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 14s 423ms/step - loss: 0.7836 - accuracy: 0.7406 - val_loss: 1.5200 - val_accuracy: 0.3919\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 14s 424ms/step - loss: 0.6290 - accuracy: 0.8241 - val_loss: 1.3533 - val_accuracy: 0.5091\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 14s 431ms/step - loss: 0.4380 - accuracy: 0.9045 - val_loss: 1.3514 - val_accuracy: 0.4970\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 13s 422ms/step - loss: 0.3009 - accuracy: 0.9520 - val_loss: 1.3751 - val_accuracy: 0.4646\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 16s 494ms/step - loss: 0.2170 - accuracy: 0.9780 - val_loss: 1.3456 - val_accuracy: 0.4929\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 14s 429ms/step - loss: 0.1747 - accuracy: 0.9845 - val_loss: 1.3746 - val_accuracy: 0.4667\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 13s 420ms/step - loss: 0.1233 - accuracy: 0.9920 - val_loss: 1.3746 - val_accuracy: 0.4970\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.0844 - accuracy: 0.9985 - val_loss: 1.4221 - val_accuracy: 0.4768\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 13s 419ms/step - loss: 0.0733 - accuracy: 0.9985 - val_loss: 1.4307 - val_accuracy: 0.4768\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 0.0556 - accuracy: 0.9995 - val_loss: 1.4084 - val_accuracy: 0.4970\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 13s 421ms/step - loss: 0.0444 - accuracy: 1.0000 - val_loss: 1.5001 - val_accuracy: 0.4949\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 13s 403ms/step - loss: 0.0389 - accuracy: 0.9995 - val_loss: 1.5726 - val_accuracy: 0.4747\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 13s 414ms/step - loss: 0.0332 - accuracy: 0.9995 - val_loss: 1.6166 - val_accuracy: 0.5010\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 13s 418ms/step - loss: 0.0288 - accuracy: 0.9995 - val_loss: 1.5045 - val_accuracy: 0.5010\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 14s 425ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 1.4672 - val_accuracy: 0.4909\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 13s 405ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 1.6004 - val_accuracy: 0.4990\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 14s 430ms/step - loss: 0.0189 - accuracy: 0.9995 - val_loss: 1.5920 - val_accuracy: 0.4929\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 10, 'lr': 0.001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 20s 955ms/step - loss: 1.6792 - accuracy: 0.3178 - val_loss: 1.7729 - val_accuracy: 0.2808\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 14s 855ms/step - loss: 1.3447 - accuracy: 0.4533 - val_loss: 1.6781 - val_accuracy: 0.3576\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 13s 840ms/step - loss: 1.1945 - accuracy: 0.5297 - val_loss: 1.5045 - val_accuracy: 0.3475\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 15s 970ms/step - loss: 1.0221 - accuracy: 0.6237 - val_loss: 1.4737 - val_accuracy: 0.3455\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 13s 863ms/step - loss: 0.7945 - accuracy: 0.7291 - val_loss: 1.4243 - val_accuracy: 0.4485\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 13s 833ms/step - loss: 0.4905 - accuracy: 0.8671 - val_loss: 1.4328 - val_accuracy: 0.4263\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 13s 833ms/step - loss: 0.2537 - accuracy: 0.9480 - val_loss: 1.4890 - val_accuracy: 0.3535\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 13s 836ms/step - loss: 0.1225 - accuracy: 0.9850 - val_loss: 1.5000 - val_accuracy: 0.3737\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 13s 843ms/step - loss: 0.0641 - accuracy: 0.9955 - val_loss: 1.4443 - val_accuracy: 0.3859\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 13s 848ms/step - loss: 0.0357 - accuracy: 0.9985 - val_loss: 1.4519 - val_accuracy: 0.3899\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 10, 'lr': 0.001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 17s 853ms/step - loss: 1.5635 - accuracy: 0.3863 - val_loss: 2.6200 - val_accuracy: 0.2970\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 13s 849ms/step - loss: 1.2303 - accuracy: 0.5197 - val_loss: 2.0374 - val_accuracy: 0.2990\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 14s 854ms/step - loss: 1.0351 - accuracy: 0.6172 - val_loss: 1.6631 - val_accuracy: 0.2949\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 13s 847ms/step - loss: 0.8055 - accuracy: 0.7291 - val_loss: 1.4777 - val_accuracy: 0.3636\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 14s 846ms/step - loss: 0.4950 - accuracy: 0.8636 - val_loss: 1.4559 - val_accuracy: 0.3717\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 13s 858ms/step - loss: 0.2288 - accuracy: 0.9610 - val_loss: 1.4418 - val_accuracy: 0.3394\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 14s 863ms/step - loss: 0.0903 - accuracy: 0.9935 - val_loss: 1.3861 - val_accuracy: 0.3879\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 13s 847ms/step - loss: 0.0411 - accuracy: 0.9980 - val_loss: 1.3786 - val_accuracy: 0.4202\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 14s 869ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 1.4120 - val_accuracy: 0.3717\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 13s 858ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 1.3813 - val_accuracy: 0.4505\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 10, 'lr': 0.0001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 19s 956ms/step - loss: 1.8463 - accuracy: 0.3058 - val_loss: 2.4773 - val_accuracy: 0.2404\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 13s 842ms/step - loss: 1.3978 - accuracy: 0.4413 - val_loss: 1.9272 - val_accuracy: 0.2465\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 13s 845ms/step - loss: 1.2190 - accuracy: 0.5277 - val_loss: 1.7252 - val_accuracy: 0.2545\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 13s 852ms/step - loss: 1.0791 - accuracy: 0.5892 - val_loss: 1.5819 - val_accuracy: 0.3111\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 13s 840ms/step - loss: 0.9203 - accuracy: 0.6797 - val_loss: 1.5344 - val_accuracy: 0.3374\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 13s 843ms/step - loss: 0.7818 - accuracy: 0.7491 - val_loss: 1.5122 - val_accuracy: 0.3657\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 14s 872ms/step - loss: 0.6527 - accuracy: 0.8271 - val_loss: 1.4913 - val_accuracy: 0.3636\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 13s 821ms/step - loss: 0.5192 - accuracy: 0.8821 - val_loss: 1.4674 - val_accuracy: 0.3859\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 13s 850ms/step - loss: 0.4207 - accuracy: 0.9280 - val_loss: 1.4607 - val_accuracy: 0.3697\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 13s 835ms/step - loss: 0.3352 - accuracy: 0.9565 - val_loss: 1.4700 - val_accuracy: 0.3616\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 10, 'lr': 0.0001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 16s 844ms/step - loss: 1.7077 - accuracy: 0.3103 - val_loss: 2.2974 - val_accuracy: 0.3091\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 13s 827ms/step - loss: 1.3421 - accuracy: 0.4613 - val_loss: 1.6007 - val_accuracy: 0.3576\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 14s 864ms/step - loss: 1.1795 - accuracy: 0.5572 - val_loss: 1.5454 - val_accuracy: 0.3455\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 16s 994ms/step - loss: 1.0305 - accuracy: 0.6352 - val_loss: 1.4662 - val_accuracy: 0.4061\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 14s 857ms/step - loss: 0.8717 - accuracy: 0.7081 - val_loss: 1.4181 - val_accuracy: 0.4202\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 14s 852ms/step - loss: 0.7244 - accuracy: 0.7746 - val_loss: 1.4379 - val_accuracy: 0.4242\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 13s 840ms/step - loss: 0.5907 - accuracy: 0.8511 - val_loss: 1.4136 - val_accuracy: 0.4384\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 15s 950ms/step - loss: 0.4459 - accuracy: 0.9060 - val_loss: 1.4395 - val_accuracy: 0.4020\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 13s 837ms/step - loss: 0.3453 - accuracy: 0.9390 - val_loss: 1.3914 - val_accuracy: 0.4182\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 14s 859ms/step - loss: 0.2588 - accuracy: 0.9655 - val_loss: 1.3991 - val_accuracy: 0.4485\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 20, 'lr': 0.001, 'num_feature_maps_cl2': 10}\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 16s 839ms/step - loss: 1.7194 - accuracy: 0.3198 - val_loss: 4.4784 - val_accuracy: 0.2040\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 13s 849ms/step - loss: 1.2719 - accuracy: 0.4873 - val_loss: 2.7884 - val_accuracy: 0.2646\n",
            "Epoch 3/20\n",
            "16/16 [==============================] - 16s 978ms/step - loss: 1.0923 - accuracy: 0.5827 - val_loss: 1.9388 - val_accuracy: 0.2909\n",
            "Epoch 4/20\n",
            "16/16 [==============================] - 13s 851ms/step - loss: 0.8523 - accuracy: 0.7131 - val_loss: 1.5933 - val_accuracy: 0.3152\n",
            "Epoch 5/20\n",
            "16/16 [==============================] - 14s 858ms/step - loss: 0.5426 - accuracy: 0.8526 - val_loss: 1.4674 - val_accuracy: 0.3273\n",
            "Epoch 6/20\n",
            "16/16 [==============================] - 13s 850ms/step - loss: 0.2786 - accuracy: 0.9475 - val_loss: 1.4646 - val_accuracy: 0.3980\n",
            "Epoch 7/20\n",
            "16/16 [==============================] - 14s 860ms/step - loss: 0.1155 - accuracy: 0.9915 - val_loss: 1.4420 - val_accuracy: 0.4020\n",
            "Epoch 8/20\n",
            "16/16 [==============================] - 14s 854ms/step - loss: 0.0501 - accuracy: 0.9990 - val_loss: 1.4148 - val_accuracy: 0.4040\n",
            "Epoch 9/20\n",
            "16/16 [==============================] - 13s 834ms/step - loss: 0.0290 - accuracy: 0.9990 - val_loss: 1.5225 - val_accuracy: 0.4000\n",
            "Epoch 10/20\n",
            "16/16 [==============================] - 13s 830ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.4852 - val_accuracy: 0.4141\n",
            "Epoch 11/20\n",
            "16/16 [==============================] - 13s 846ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 1.4831 - val_accuracy: 0.3939\n",
            "Epoch 12/20\n",
            "16/16 [==============================] - 13s 843ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 1.4701 - val_accuracy: 0.4101\n",
            "Epoch 13/20\n",
            "16/16 [==============================] - 14s 855ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 1.4751 - val_accuracy: 0.4020\n",
            "Epoch 14/20\n",
            "16/16 [==============================] - 14s 865ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.4890 - val_accuracy: 0.4162\n",
            "Epoch 15/20\n",
            "16/16 [==============================] - 13s 839ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 1.5141 - val_accuracy: 0.4061\n",
            "Epoch 16/20\n",
            "16/16 [==============================] - 13s 847ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 1.5162 - val_accuracy: 0.4242\n",
            "Epoch 17/20\n",
            "16/16 [==============================] - 13s 835ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.5509 - val_accuracy: 0.4263\n",
            "Epoch 18/20\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.6149 - val_accuracy: 0.4162\n",
            "Epoch 19/20\n",
            "16/16 [==============================] - 13s 835ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.6128 - val_accuracy: 0.4242\n",
            "Epoch 20/20\n",
            "16/16 [==============================] - 13s 839ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.6571 - val_accuracy: 0.4263\n",
            "Found 2001 images belonging to 5 classes.\n",
            "Found 495 images belonging to 5 classes.\n",
            "Training with params: {'batch_size': 128, 'epochs': 20, 'lr': 0.001, 'num_feature_maps_cl2': 20}\n",
            "Epoch 1/20\n",
            "16/16 [==============================] - 16s 857ms/step - loss: 1.5676 - accuracy: 0.3573 - val_loss: 4.3744 - val_accuracy: 0.2081\n",
            "Epoch 2/20\n",
            "16/16 [==============================] - 14s 862ms/step - loss: 1.2330 - accuracy: 0.4963 - val_loss: 2.8450 - val_accuracy: 0.2162\n",
            "Epoch 3/20\n",
            " 6/16 [==========>...................] - ETA: 6s - loss: 1.1308 - accuracy: 0.5534"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b41652f1af91>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-94776916e0fa>\u001b[0m in \u001b[0;36mperform_grid_search\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         history = model.fit(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mtraingen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "best_model = load_model('drive/MyDrive/task_1/best_model_task_2.keras)"
      ],
      "metadata": {
        "id": "i8n69LU9WFxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc(best_model,testgen,1)"
      ],
      "metadata": {
        "id": "D8I4InLVY2GQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}